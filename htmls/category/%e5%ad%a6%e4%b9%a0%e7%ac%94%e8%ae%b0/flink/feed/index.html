<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>flink &#8211; 三两带走</title>
	<atom:link href="http://rentb.vicp.net/htmls/category/%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/flink/feed" rel="self" type="application/rss+xml" />
	<link>http://huster.top/</link>
	<description>(任天兵)龙安的博客</description>
	<lastBuildDate>Sun, 28 Jun 2020 08:13:56 +0000</lastBuildDate>
	<language>zh-CN</language>
	<sy:updatePeriod>hourly</sy:updatePeriod>
	<sy:updateFrequency>1</sy:updateFrequency>
	<generator>https://wordpress.org/?v=4.9.10</generator>

<image>
	<url>https://huster.top/wp-content/uploads/2018/09/cropped-fish_112.18181818182px_1208536_easyicon.net_-32x32.png</url>
	<title>flink &#8211; 三两带走</title>
	<link>http://huster.top/</link>
	<width>32</width>
	<height>32</height>
</image> 
	<item>
		<title>flink读取kafka数据未能触发watermark</title>
		<link>https://huster.top/htmls/713.html</link>
		<comments>https://huster.top/htmls/713.html#respond</comments>
		<pubDate>Wed, 05 Feb 2020 10:33:25 +0000</pubDate>
		<dc:creator><![CDATA[龙安_任天兵]]></dc:creator>
				<category><![CDATA[flink]]></category>

		<guid isPermaLink="false">https://huster.top/?p=713</guid>
		<description><![CDATA[过年被病毒闹的出不了门，就顺带着把flink秒级统计的逻辑写了一下，逻辑很简单，就是从kafka消费数据，然后 &#8230; <a href="https://huster.top/htmls/713.html" class="more-link">继续阅读<span class="screen-reader-text">“flink读取kafka数据未能触发watermark”</span></a>]]></description>
				<content:encoded><![CDATA[<p>过年被病毒闹的出不了门，就顺带着把flink秒级统计的逻辑写了一下，逻辑很简单，就是从kafka消费数据，然后select count(*) from xxx group by xxx来一秒产出一个打点。程序代码非常简单</p>
<p><span id="more-713"></span></p>
<p>&#8220;`java</p>
<pre>StreamExecutionEnvironment bsEnv = StreamExecutionEnvironment.getExecutionEnvironment();
EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();
StreamTableEnvironment bsTableEnv = StreamTableEnvironment.create(bsEnv, bsSettings);
bsEnv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);
//并发数量小于等于kafka的partition数量,否则不会触发水位线
bsEnv.setParallelism(1);
//get input from kafka
String[] fieldNamesOriginal = new String[]{"i","t","op"};
TypeInformation[] typesOriginal = new TypeInformation[]{Types.STRING(), Types.JAVA_BIG_DEC(), Types.STRING()};
Properties properties = new Properties();
properties.setProperty("bootstrap.servers", "xxxx.com:9200");
properties.setProperty("group.id", "xxxxxxx");
//主动提交offset,不利用flink的job
properties.put("enable.auto.commit", "true");
properties.put("auto.commit.interval.ms", "1000");
FlinkKafkaConsumer011&lt;Row&gt; kafkaConsumer011 = new FlinkKafkaConsumer011&lt;&gt;("xxxxxxxx",
        new JsonRowDeserializationSchema(new RowTypeInfo(typesOriginal, fieldNamesOriginal)), properties);
kafkaConsumer011.setStartFromLatest();
DataStream&lt;Row&gt; streamSource = bsEnv
        .addSource(kafkaConsumer011)
        .uid("from_kafka_topic_is");
TypeInformation[] returnTypeTmp = new TypeInformation[]{Types.STRING(), Types.SQL_TIME(), Types.STRING()};
DataStream&lt;Row&gt; dataStreamWithTime = streamSource.map(new MapFunction&lt;Row, Row&gt;() {
    @Override
    public Row map(Row value) throws Exception {
        //SimpleDateFormat simpleDateFormat = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
        Row row = new Row(3);
        BigDecimal time = (BigDecimal)value.getField(1);
        row.setField(0, value.getField(0));
        row.setField(1, new Time(time.longValue()));
        row.setField(2, value.getField(2));
        return row;
    }
})
        .returns(new RowTypeInfo(returnTypeTmp, fieldNamesOriginal))
        .uid("add_time")
        .assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor&lt;Row&gt;(org.apache.flink.streaming.api.windowing.time.Time.seconds(2)) {
            @Override
            public long extractTimestamp(Row element) {
                return ((Time)element.getField(1)).getTime();
            }
        });
bsTableEnv.registerDataStream("_source_table", dataStreamWithTime, "i,t,op, rowtime.rowtime");
bsTableEnv.connect(new Kafka()
        .version("0.11")
        .topic("s-log")
        .property("bootstrap.servers", "abc.com:9200")
        .startFromEarliest())
        .withFormat(new Json().jsonSchema("{\"type\":\"object\",\"properties\":{\"index_name\":{\"type\":\"string\"},\"event_time_log\":{\"type\":\"string\"},\"op_type\":{\"type\":\"string\"},\"count_result\":{\"type\":\"number\"}}}"))
        .withSchema(new Schema()
                .field("index_name", Types.STRING())
                .field("event_time_log", Types.STRING())
                .field("op_type", Types.STRING())
                .field("count_result", Types.LONG()))
        .inAppendMode()
        .registerTableSink("operator_log_result");
bsTableEnv.sqlUpdate("insert into operator_log_result " +
        "select  i as index_name,  DATE_FORMAT(TUMBLE_END(rowtime, INTERVAL '1' SECOND), 'yyyy-MM-dd HH:mm:ss') as event_time_log, op as op_type, count(op) as count_result " +
        "from _source_table group by TUMBLE(rowtime, INTERVAL '1' SECOND) , i, op");
bsEnv.execute("data-metrics");




</pre>
<p>代码如上，非常简单，输入的数据类似于</p>
<p>&#8220;`json</p>
<p class="p1"><span class="s1">{&#8220;t&#8221;:1580896327330,&#8221;st&#8221;:&#8221;DIRECT&#8221;,&#8221;i&#8221;:&#8221;hitch_passenger_order_2020_01_22_23_50_40&#8243;,&#8221;op&#8221;:&#8221;es_sink&#8221;,&#8221;id&#8221;:&#8221;,guid:15756341902141200002900&#8243;,&#8221;ut&#8221;:&#8221;,order_update_time:2020-02-05 17:52:07.190435&#8243;,&#8221;msg&#8221;:&#8221;&#8221;,&#8221;rId&#8221;:&#8221;0.1.1.1&#8243;}</span></p>
<p class="p1"><span class="s1">{&#8220;t&#8221;:1580896327831,&#8221;st&#8221;:&#8221;DIRECT&#8221;,&#8221;i&#8221;:&#8221;hitch_passenger_order_2020_01_22_23_50_40&#8243;,&#8221;op&#8221;:&#8221;es_sink&#8221;,&#8221;id&#8221;:&#8221;,guid:15756341902141200002900&#8243;,&#8221;ut&#8221;:&#8221;,order_update_time:2020-02-05 17:52:07.687468&#8243;,&#8221;msg&#8221;:&#8221;&#8221;,&#8221;rId&#8221;:&#8221;0.1.1.1&#8243;}</span></p>
<p class="p1"><span class="s1">{&#8220;t&#8221;:1580896328131,&#8221;st&#8221;:&#8221;DIRECT&#8221;,&#8221;i&#8221;:&#8221;hitch_passenger_order_2020_01_22_23_50_40&#8243;,&#8221;op&#8221;:&#8221;es_sink&#8221;,&#8221;id&#8221;:&#8221;,guid:15756341902141200002900&#8243;,&#8221;ut&#8221;:&#8221;,order_update_time:2020-02-05 17:52:08.047474&#8243;,&#8221;msg&#8221;:&#8221;&#8221;,&#8221;rId&#8221;:&#8221;0.1.1.1&#8243;}</span></p>
<p>&nbsp;</p>
<p>然而，死活就是无法输出正确的统计数据。但是当我把setParallelism并发设置为1的时候，就能正常输出统计数据。太神奇了，为什么跟并发度有关系呢？</p>
<p>于是我在本地打开了跟踪调试功能，我发现问题出在StatusWatermarkValve这个类里，这个类的这个方法是这么写的</p>
<p>&#8220;`java</p>
<pre>private void findAndOutputNewMinWatermarkAcrossAlignedChannels() {
   long newMinWatermark = Long.MAX_VALUE;
   boolean hasAlignedChannels = false;

   // determine new overall watermark by considering only watermark-aligned channels across all channels
   for (InputChannelStatus channelStatus : channelStatuses) {
      if (channelStatus.isWatermarkAligned) {
         hasAlignedChannels = true;
         newMinWatermark = Math.min(channelStatus.watermark, newMinWatermark);
      }
   }

   // we acknowledge and output the new overall watermark if it really is aggregated
   // from some remaining aligned channel, and is also larger than the last output watermark
   if (hasAlignedChannels &amp;&amp; newMinWatermark &gt; lastOutputWatermark) {
      lastOutputWatermark = newMinWatermark;
      outputHandler.handleWatermark(new Watermark(lastOutputWatermark));
   }
}</pre>
<p>这个方法跟踪调试，正常的时候是这样的也是比较符合我们预期的，到点了就该触发了。</p>
<p><a href="https://huster.top/wp-content/uploads/2020/02/b1.png"><img class="alignnone size-full wp-image-714" src="https://huster.top/wp-content/uploads/2020/02/b1.png" alt="" width="2432" height="474" /></a></p>
<p>而不正常的时候，多个并发的时候，是这样的</p>
<p><a href="https://huster.top/wp-content/uploads/2020/02/b2.png"><img class="alignnone size-full wp-image-715" src="https://huster.top/wp-content/uploads/2020/02/b2.png" alt="" width="2384" height="982" /></a></p>
<p>可以看到其他的subtask取到的watermark是一个负值，也就是初始值。</p>
<p>&nbsp;</p>
<p>这个时候flink怎么处理的呢？根据刚刚的代码，他会取并发里面最小的一个值作为watermark，也就是那个最小的long型值。</p>
<p>然后一比较，跟默认值等的，不满足触发的条件，也就未能触发事件了。</p>
<p>看到一篇文章是这么写的</p>
<p><a href="https://huster.top/wp-content/uploads/2020/02/a3.jpg"><img class="alignnone size-full wp-image-716" src="https://huster.top/wp-content/uploads/2020/02/a3.jpg" alt="" width="1518" height="1400" /></a></p>
<p>&nbsp;</p>
<p>文章引用： https://www.jianshu.com/p/753e8cf803bb</p>
<p>&nbsp;</p>
<p>我感觉是flink的代码不合理导致的，至此，困扰我多日的一个问题终于有了答案。</p>
]]></content:encoded>
			<wfw:commentRss>https://huster.top/htmls/713.html/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>flink使用时间戳作为起始offset消费kafka的问题</title>
		<link>https://huster.top/htmls/708.html</link>
		<comments>https://huster.top/htmls/708.html#respond</comments>
		<pubDate>Mon, 21 Oct 2019 15:59:05 +0000</pubDate>
		<dc:creator><![CDATA[龙安_任天兵]]></dc:creator>
				<category><![CDATA[flink]]></category>

		<guid isPermaLink="false">https://huster.top/?p=708</guid>
		<description><![CDATA[代码如下 private static voidtest2()throwsException {     En &#8230; <a href="https://huster.top/htmls/708.html" class="more-link">继续阅读<span class="screen-reader-text">“flink使用时间戳作为起始offset消费kafka的问题”</span></a>]]></description>
				<content:encoded><![CDATA[<div>代码如下</div>
<div></div>
<div></div>
<div>private static voidtest2()throwsException {</div>
<div>    EnvironmentSettings fsSettings = EnvironmentSettings.newInstance().useOldPlanner().inStreamingMode().build();</div>
<div>   StreamExecutionEnvironment fsEnv = StreamExecutionEnvironment.getExecutionEnvironment();</div>
<div>   StreamTableEnvironment tttt = StreamTableEnvironment.create(fsEnv,fsSettings);</div>
<div>   Properties properties =newProperties();</div>
<div>   properties.setProperty(&#8220;bootstrap.servers&#8221;,&#8221;localhost:9092&#8243;);</div>
<div>    properties.setProperty(&#8220;<a href="http://group.id/">group.id</a>&#8220;,&#8221;test&#8221;+ System.currentTimeMillis());</div>
<div>   FlinkKafkaConsumer011&lt;String&gt; kafkaConsumer011 =newFlinkKafkaConsumer011&lt;String&gt;(&#8220;test&#8221;, newSimpleStringSchema(),properties);</div>
<div>   kafkaConsumer011.setStartFromTimestamp(System.currentTimeMillis());</div>
<div>   DataStream&lt;String&gt; stream = fsEnv.addSource(kafkaConsumer011);</div>
<div>   stream.print();</div>
<div>   fsEnv.execute(&#8220;test&#8221;);</div>
<div>}2019-10-21 23:06:58.970 [Thread-8] INFO  o.a.f.s.connectors.kafka.FlinkKafkaConsumerBase &#8211; Consumer subtask 0 creating fetcher with offsets {}.</div>
<div></div>
<div>表现为，无法收到kafka发送过来的消息。这就很奇怪了，因为我使用kafka-consumer-console.sh的方式可以看到实际上是有消息产出的，但是这里死活就收不到数据。</div>
<div></div>
<p><span id="more-708"></span></p>
<div></div>
<div>进一步的测试，我把timestamp改成提前了很长的时间，就能正常工作。</div>
<div></div>
<div>进一步测试，如果刚好设置的timestamp那个点有消息产生，那么也能正常工作。</div>
<div></div>
<div></div>
<div>对比了下，能拿到数据和不能拿到数据的区别。下面是不能拿到数据时候的日志输出，</div>
<div></div>
<div></div>
<div>2019-10-21 23:06:58.970 [Thread-10] INFO  o.a.f.s.connectors.kafka.FlinkKafkaConsumerBase &#8211; Consumer subtask 3 creating fetcher with offsets {}.</div>
<div>2019-10-21 23:06:58.970 [Thread-7] INFO  o.a.f.s.connectors.kafka.FlinkKafkaConsumerBase &#8211; Consumer subtask 1 creating fetcher with offsets {}.</div>
<div>2019-10-21 23:07:01.100 [Source: Custom Source -&gt; Sink: Print to Std. Out (3/4)] INFO  o.a.f.s.connectors.kafka.FlinkKafkaConsumerBase &#8211; Consumer subtask 2 initially has no partitions to read from.</div>
<div>2019-10-21 23:07:01.108 [Thread-9] INFO  o.a.f.s.connectors.kafka.FlinkKafkaConsumerBase &#8211; Consumer subtask 2 creating fetcher with offsets {}.</div>
<div></div>
<div></div>
<div>而下面是能拿到数据时候的日志输出, 可以看到有一个task成功拿到了这个partition的offset</div>
<div></div>
<div></div>
<div>2019-10-21 23:14:11.871 [Thread-7] INFO  o.a.f.s.connectors.kafka.FlinkKafkaConsumerBase &#8211; Consumer subtask 2 creating fetcher with offsets {KafkaTopicPartition{topic=&#8217;test&#8217;, partition=0}=35}.</div>
<div></div>
<div></div>
<div>所以原因是因为，通过kafka的consumer的api，consumer.offsetsForTimes的时候，能否拿到offset，如果拿不到在KafkaConsumerThread的线程的run方法里，有一个这个代码表明这个方法是阻塞的，一直等到有新的partition到来，才会继续。而新的partition不会到来，所以这里就有问题了。。。</div>
<div></div>
<div>try {</div>
<div>   if (hasAssignedPartitions) {</div>
<div>      newPartitions = unassignedPartitionsQueue.pollBatch();</div>
<div>   }</div>
<div>   else {</div>
<div>      // if no assigned partitions block until we get at least one</div>
<div>      // instead of hot spinning this loop. We rely on a fact that</div>
<div>      // unassignedPartitionsQueue will be closed on a shutdown, so</div>
<div>      // we don&#8217;t block indefinitely</div>
<div>      newPartitions = unassignedPartitionsQueue.getBatchBlocking();</div>
<div>   }</div>
<div>   if (newPartitions != null) {</div>
<div>      reassignPartitions(newPartitions);</div>
<div>   }</div>
<div>} catch (AbortedReassignmentException e) {</div>
<div>   continue;</div>
<div>}</div>
<div></div>
<div></div>
<div></div>
<div></div>
<div></div>
<div>那么kafka的这个offsetsForTimes的接口为何不能返回结果呢 ？</div>
<div></div>
<div>private static void testConsumer() {</div>
<div>    Properties props = new Properties();</div>
<div>    props.put(&#8220;bootstrap.servers&#8221;, &#8220;localhost:9092&#8221;);</div>
<div>    props.put(&#8220;group.id&#8221;, &#8220;test&#8221;);</div>
<div>    props.put(&#8220;key.deserializer&#8221;, &#8220;org.apache.kafka.common.serialization.StringDeserializer&#8221;);</div>
<div>    props.put(&#8220;value.deserializer&#8221;, &#8220;org.apache.kafka.common.serialization.StringDeserializer&#8221;);</div>
<div>    KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props);</div>
<div>    consumer.subscribe(Collections.singletonList(&#8220;test&#8221;));</div>
<div>    TopicPartition topicPartition = new TopicPartition(&#8220;test&#8221;, 0);</div>
<div>    long current = System.currentTimeMillis();</div>
<div>    for (int i=0; i&lt;10000; i++) {</div>
<div>        Map&lt;TopicPartition, Long&gt; map = new HashMap&lt;&gt;();</div>
<div>        map.put(topicPartition, current &#8211; i*1000);</div>
<div>        Map&lt;TopicPartition, OffsetAndTimestamp&gt; result  = consumer.offsetsForTimes(map);</div>
<div>        System.out.println(&#8220;===================test : &#8221; + i + &#8220;==================&#8221;);</div>
<div>        System.out.println(new Gson().toJson(result));</div>
<div>    }</div>
<div>    while (true) {</div>
<div>        ConsumerRecords&lt;String, String&gt; records = consumer.poll(100);</div>
<div>        for (ConsumerRecord&lt;String, String&gt; record : records)</div>
<div>            System.out.printf(&#8220;offset = %d, key = %s, value = %s%n&#8221;, record.offset(), record.key(), record.value());</div>
<div>    }</div>
<div>}</div>
<div></div>
<div></div>
<div>通过如上代码我们发现，如果这个timestamp之后有数据产出，那么就能正确的拿到offset，否则就拿不到。</div>
<div></div>
<div>也就是我们最后一条数据如果是18：00产生的，那么如果你的时间戳早于18:00是能正确的拿到offset的，如果比18:00晚，那么就拿不到了。</div>
<div></div>
<div></div>
<div>总结，问题在于flink和kafka的配合。kafka的特征是，时间戳之后如果没有消息，那么就不返回offset(我认为合理的应该是返回last offset + 1) , 那么flink发现没有返回offset是怎么处理的呢？那就不消费数据了，因为他觉得返回的数据并不符合自己的预期，他也不知道从哪里开始取了。如果从offset+1开始取，可能会出错，干脆就不取了。</div>
<div></div>
]]></content:encoded>
			<wfw:commentRss>https://huster.top/htmls/708.html/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>flink通过jdbc读取postgresql数据库里的数据</title>
		<link>https://huster.top/htmls/706.html</link>
		<comments>https://huster.top/htmls/706.html#respond</comments>
		<pubDate>Thu, 17 Oct 2019 08:49:32 +0000</pubDate>
		<dc:creator><![CDATA[龙安_任天兵]]></dc:creator>
				<category><![CDATA[flink]]></category>

		<guid isPermaLink="false">https://huster.top/?p=706</guid>
		<description><![CDATA[问题1： 从postgresql里使用flink-jdbc读取数据的问题, 数据类型不匹配，不支持jsonb等 &#8230; <a href="https://huster.top/htmls/706.html" class="more-link">继续阅读<span class="screen-reader-text">“flink通过jdbc读取postgresql数据库里的数据”</span></a>]]></description>
				<content:encoded><![CDATA[<div></div>
<div></div>
<div></div>
<div>问题1： 从postgresql里使用flink-jdbc读取数据的问题, 数据类型不匹配，不支持jsonb等其他类型.</div>
<div></div>
<div></div>
<p><span id="more-706"></span></p>
<div></div>
<div>        使用create table 语句来读取，由于postgresql里设置的是jsonb的类型, 他又不让你输入sql语句(语句里需要写CAST函数),所以就抛错了</div>
<div>Caused by: java.lang.ClassCastException: org.postgresql.util.PGobject cannot be cast to java.lang.String</div>
<div>Caused by: java.lang.ClassCastException: org.postgresql.util.PGobject cannot be cast to java.lang.String</div>
<div>    at <a href="http://org.apache.flink.api.common.typeutils.base.stringserializer.copy(stringserializer.java:33/">org.apache.flink.api.common.typeutils.base.StringSerializer.copy(StringSerializer.java:33</a>)</div>
<div>    at org.apache.flink.api.java.typeutils.runtime.RowSerializer.copy(RowSerializer.java:93)</div>
<div>    at org.apache.flink.api.java.typeutils.runtime.RowSerializer.copy(RowSerializer.java:44)</div>
<div>    at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:635)</div>
<div>    &#8230; 9 more</div>
<div></div>
<div></div>
<div>问题2: partition必须是数字类型的字段</div>
<div></div>
<div></div>
<div> 另外，因为connector.read.partition.column他使用的是between语句，所以只能是数字，时间戳等，否则也会报错</div>
<div>. partition.column must be a numeric,</div>
<div> &#8212; date, or timestamp column from the table in question.</div>
<div></div>
<div></div>
<div></div>
<div></div>
<div></div>
<div></div>
<div>Exception in thread &#8220;main&#8221; org.apache.flink.runtime.client.JobExecutionException: Job execution failed.</div>
<div>    at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:146)</div>
<div>    at org.apache.flink.runtime.minicluster.MiniCluster.executeJobBlocking(MiniCluster.java:627)</div>
<div>    at org.apache.flink.streaming.api.environment.LocalStreamEnvironment.execute(LocalStreamEnvironment.java:117)</div>
<div>    at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1507)</div>
<div>    at com.hellobike.search.flink.task.FullIndexBuilderTask.main(FullIndexBuilderTask.java:92)</div>
<div>Caused by: java.lang.Exception: java.lang.IllegalArgumentException: open() failed.ERROR: operator does not exist: character varying &gt;= bigint</div>
<div>  建议：No operator matches the given name and argument type(s). You might need to add explicit type casts.</div>
<div>  位置：514</div>
<div>    at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.checkThrowSourceExecutionException(SourceStreamTask.java:212)</div>
<div>    at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.performDefaultAction(SourceStreamTask.java:132)</div>
<div>    at org.apache.flink.streaming.runtime.tasks.StreamTask.run(StreamTask.java:298)</div>
<div>    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:403)</div>
<div>    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:705)</div>
<div>    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:530)</div>
<div>    at java.lang.Thread.run(Thread.java:748)</div>
<div>Caused by: java.lang.IllegalArgumentException: open() failed.ERROR: operator does not exist: character varying &gt;= bigint</div>
<div>  建议：No operator matches the given name and argument type(s). You might need to add explicit type casts.</div>
<div>  位置：514</div>
<div>    at org.apache.flink.api.java.io.jdbc.JDBCInputFormat.open(JDBCInputFormat.java:250)</div>
<div>    at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:85)</div>
<div>    at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:100)</div>
<div>    at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:63)</div>
<div>    at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:202)</div>
<div>Caused by: org.postgresql.util.PSQLException: ERROR: operator does not exist: character varying &gt;= bigint</div>
<div>  建议：No operator matches the given name and argument type(s). You might need to add explicit type casts.</div>
<div>  位置：514</div>
<div>    at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2103)</div>
<div>    at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:1836)</div>
<div>    at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:257)</div>
<div>    at org.postgresql.jdbc2.AbstractJdbc2Statement.execute(AbstractJdbc2Statement.java:512)</div>
<div>    at org.postgresql.jdbc2.AbstractJdbc2Statement.executeWithFlags(AbstractJdbc2Statement.java:388)</div>
<div>    at org.postgresql.jdbc2.AbstractJdbc2Statement.executeQuery(AbstractJdbc2Statement.java:273)</div>
<div>    at org.apache.flink.api.java.io.jdbc.JDBCInputFormat.open(JDBCInputFormat.java:247)</div>
<div>    &#8230; 4 more</div>
<div>
<div></div>
</div>
]]></content:encoded>
			<wfw:commentRss>https://huster.top/htmls/706.html/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>flink相关 &#8211; 通过Table API写入ElasticSearch的部分源码分析</title>
		<link>https://huster.top/htmls/704.html</link>
		<comments>https://huster.top/htmls/704.html#respond</comments>
		<pubDate>Thu, 17 Oct 2019 08:49:06 +0000</pubDate>
		<dc:creator><![CDATA[龙安_任天兵]]></dc:creator>
				<category><![CDATA[flink]]></category>

		<guid isPermaLink="false">https://huster.top/?p=704</guid>
		<description><![CDATA[New ElasticSearch()…xxx&#8230;registerTableSink(); Elas &#8230; <a href="https://huster.top/htmls/704.html" class="more-link">继续阅读<span class="screen-reader-text">“flink相关 &#8211; 通过Table API写入ElasticSearch的部分源码分析”</span></a>]]></description>
				<content:encoded><![CDATA[<div>New ElasticSearch()…xxx&#8230;registerTableSink();</div>
<div></div>
<div>ElasticSearch是一个ConnectorDescriptor: 最关键的就是一个toConnectorProperties就是一个配置的map,传给factory使用，他其实是一个配置收集器.</div>
<div></div>
<div></div>
<p><span id="more-704"></span></p>
<div></div>
<div>TableFactoryUtils :  生成一个TableFactory的工具类，</div>
<div>findAndCreateTableSink( ) 通过一个描述来生成一个工厂类.//描述里有必要的配置.</div>
<div></div>
<div>先找到合适的工厂类，然后创建一个TableSink.</div>
<div></div>
<div>第一步，A先找到合适的工厂类，通过properties里的配置来寻找,</div>
<div>TableFactoryService.find(TableSinkFactory.class, properties)</div>
<div>findSingleInternal()里的两个代码是关键代码</div>
<div></div>
<div>A.1 : List&lt;TableFactory&gt; tableFactories = discoverFactories(classLoader);    通过classLoader(可以传入, 否则使用默认)找到所有的Factory.</div>
<div>A.2:  然后开始过滤， List&lt;T&gt; filtered = filter(tableFactories, factoryClass, properties)</div>
<div>        A.2.1 : 首先检查类型, TableSinkFactory.class   filterByFactoryClass(factoryClass,properties,foundFactories)</div>
<div>        A.2.2 : 然后检查必须的配置是不是设置了, List&lt;T&gt; contextFactories = filterByContext(factoryClass,properties,foundFactories,classFactories);</div>
<div>                    其中必备的配置是在factory里配置和定义的，必须不能为空.这个是有TableFactory的requiredContext()方法定义的.</div>
<div>       A.2.3 : 经过以上两轮的过滤，然后再继续检查，依然是TableFactoryService.java，这里面的</div>
<div>   private static &lt;T extends TableFactory&gt; List&lt;T&gt; filterBySupportedProperties(</div>
<div>            Class&lt;T&gt; factoryClass,</div>
<div>            Map&lt;String, String&gt; properties,</div>
<div>            List&lt;TableFactory&gt; foundFactories,</div>
<div>            List&lt;T&gt; classFactories) 方法</div>
<div></div>
<div>            这里主要是TableFactory里的两个方法定义的属性</div>
<div>                    Map&lt;String, String&gt; requiredContext();</div>
<div>                    List&lt;String&gt; supportedProperties();</div>
<div>            然后和用户Descriptor（如 ElasticSearch）里设置的属性匹配，其中有字符忽略大小写，星号匹配等细节，不再研究.</div>
<div></div>
<div>然后这就找到了工厂类，接下来要生成TableSink了。例如 ElasticSearch找到的就是Elasticsearch6UpsertTableSinkFactory.</div>
<div></div>
<div>第二步，生成TableSink.在ElasticSearch中，他是直接的new了一个Elasticsearch6UpsertTableSink.而这个对象又是一个StreamTableSink，这样就生成了一个StreamTableSink, 这个sink可以交给flink直接使用了。在ElasticSearch这个TableSink里，最关键的是实现了consumeDataStream，这个决定了sink的写逻辑.注意区分的是一个是StreamTableSink是给Table接口使用的，另一个是DataStreamSink是给DataStream使用的。这里是连接Table和Stream接口的地方。所以说，Table的API其实还是基于Stream的API的。</div>
<div></div>
<div></div>
<div>生成了Elasticsearch的TableSink之后，可以看到到这个类的用途是为了向es写入数据，他的作用是生成一个createSinkFunction。</div>
<div>createSinkFunction是具体的连接es，向es写入数据的逻辑，具体的实现是ElasticsearchUpsertSinkFunction的process的方法。是由ElasticsearchSinkBase发起的调用。</div>
<div></div>
<div></div>
<div></div>
<div>在写入ElasticSearch的时候，如何让_id使用数据里的指定的field的值？</div>
<div>在ES2.0之后，mapping里设置_id就被移除了这个特性。目的是为了让业务方不需要关心es的实现细节。在es connector里，他会根据你的table的primary key来生成这个_id的值。一般情况下，在append模式下，因为只是insert，所以他都是随机的。只有在update的时候，才需要根据primary key查找到元素并且进行更新。具体的实现在processUpsert方法。</div>
<div></div>
<div>        private void processUpsert(Row row, RequestIndexer indexer) {</div>
<div>            final byte[] document = serializationSchema.serialize(row);</div>
<div>            if (keyFieldIndices.length == 0) {</div>
<div>                final IndexRequest indexRequest = requestFactory.createIndexRequest(</div>
<div>                    index,</div>
<div>                    docType,</div>
<div>                    contentType,</div>
<div>                    document);</div>
<div>                indexer.add(indexRequest);</div>
<div>            } else {</div>
<div>                final String key = createKey(row);</div>
<div>                final UpdateRequest updateRequest = requestFactory.createUpdateRequest(</div>
<div>                    index,</div>
<div>                    docType,</div>
<div>                    key,</div>
<div>                    contentType,</div>
<div>                    document);</div>
<div>                indexer.add(updateRequest);</div>
<div>            }</div>
<div>        }</div>
<div></div>
<div>这个方法的关键是keyFieldIndices这个属性。如果这个属性没值，那么久不指定key，让他自动生成_id，否则就从元数据里拿到这个字段值，再拼接成_id.而这个值是由方法</div>
<div>public void setKeyFields(String[] keyNames) {} 来设置的。这个就是tablesink接口里定义的方法。这个方法可不是给业务方调用的，这个是flink框架调用的。所以你显示的调用这个方法也没用，他flink最终会覆盖掉你的配置。</div>
<div>/**</div>
<div>* Configures the unique key fields of the {@linkTable} to write.</div>
<div>* The method is called after {@linkTableSink#configure(String[], TypeInformation[])}.</div>
<div>*</div>
<div>*&lt;p&gt;The keys array might be empty, if the table consists of a single (updated) record.</div>
<div>* If the table does not have a key and is append-only, the keys attribute is null.</div>
<div>*</div>
<div>*@paramkeysthe field names of the table&#8217;s keys, an empty array if the table has a single</div>
<div>*             row, and null if the table is append-only and has no key.</div>
<div>*/</div>
<div>voidsetKeyFields(String[] keys);</div>
<div></div>
<div>说明在这里。说的很清楚了，如果是append-only的模式，传入的keys就为空，否则就用table 的 primary key. 所以你可以定义一个table，指定主键。</div>
<div></div>
<div>找了一圈发现并没有定义table 的 primary的语法，翻看flink的源码发现他是自动判断的，具体的是根据你的sql语句来判断的。</div>
<div></div>
<div>UpdatingPlanChecker.scala是一个scala的不太好看明白，具体的在.private class UniqueKeyExtractor {} 这个类里面。他的寻找方式是，如果你是group by 语句，那么group by 就是你的key.如果你是join，那就看你左边的表和你右边的表来分别递归计算。</div>
<div></div>
<div>// Output of join must have keys if left and right both contain key(s).</div>
<div>// Key groups from both side will be merged by join equi-predicates</div>
<div>val lInNames: Seq[String] = j.getLeft.getRowType.getFieldNames</div>
<div>val rInNames: Seq[String] = j.getRight.getRowType.getFieldNames</div>
<div>val joinNames = j.getRowType.getFieldNames</div>
<div>// if right field names equal to left field names, calcite will rename right</div>
<div>// field names. For example, T1(pk, a) join T2(pk, b), calcite will rename T2(pk, b)</div>
<div></div>
<div></div>
<div>总之就是，只有在聚合计算的时候才会有key, 也就是group by 后面的内容。那这不就扯了吗？ ElasticSearch connector 不就只支持插入操作了吗？</div>
<div></div>
<div></div>
<div>Ps： flink后续会支持在table schema里定义primary key, 已经提交了pr，<a href="https://github.com/apache/flink/pull/8736">https://github.com/apache/flink/pull/8736</a> 可惜现在还没有。pat ~</div>
<div></div>
<div></div>
<div></div>
<div></div>
<div></div>
<div></div>
<div></div>
]]></content:encoded>
			<wfw:commentRss>https://huster.top/htmls/704.html/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>flink的动态表格</title>
		<link>https://huster.top/htmls/700.html</link>
		<comments>https://huster.top/htmls/700.html#respond</comments>
		<pubDate>Thu, 17 Oct 2019 08:48:28 +0000</pubDate>
		<dc:creator><![CDATA[龙安_任天兵]]></dc:creator>
				<category><![CDATA[flink]]></category>

		<guid isPermaLink="false">https://huster.top/?p=700</guid>
		<description><![CDATA[目的： 订阅kafka的消息(kafka的消息是从postgresql来的)，将kafka的消息作为Table &#8230; <a href="https://huster.top/htmls/700.html" class="more-link">继续阅读<span class="screen-reader-text">“flink的动态表格”</span></a>]]></description>
				<content:encoded><![CDATA[<div>目的： 订阅kafka的消息(kafka的消息是从postgresql来的)，将kafka的消息作为TableSource，执行sql，将结果输出到ElasticSearch中</div>
<div></div>
<div>动态表格</div>
<div></div>
<div>    flink的概念里，stream的输入是一个insert的模式。类似于点击日志这种，是源源不断的产出的新数据。而postgresql到kafka的消息包含了insert,update和delete的操作。</div>
<div>在点击日志的概率里，如果统计点击次数，比较好理解。一种是表格不断的更新(update mode)。如果unique key第一次出现，则插入，否则做累加，类似下面这种</div>
<div><a href="https://huster.top/wp-content/uploads/2019/10/5.png"><img class="alignnone size-full wp-image-701" src="https://huster.top/wp-content/uploads/2019/10/5.png" alt="" width="850" height="370" /></a></div>
<div></div>
<div>还有一种则是基于窗口的统计，每一个窗口期插入一系列的数据，所以这个是insert模式</div>
<div><a href="https://huster.top/wp-content/uploads/2019/10/6.png"><img class="alignnone size-full wp-image-702" src="https://huster.top/wp-content/uploads/2019/10/6.png" alt="" width="872" height="350" /></a></div>
<div></div>
<div></div>
<div></div>
<div>但是在我们的场景里面，kafka里的消息不仅仅是插入的数据，还有可能是更新的数据。也就是有个维表，里面的数据在不断的更新。</div>
<div></div>
<div>那么现在就是要搞清楚一件事。如果我定义了一个Table，接受Stream data数据不断流入。如果我这个table里不存在这表数据(定义primary key)，那么他就会执行插入。否则他会执行update操作。我们写代码来试验一下这个想法， 看看是不是这样。如果没有group by 语句，你连primary key 都定义不了，也就没办法实现更新了…..所以要区分情况，如果你是上面的例子，通过group by 计算count, 因为有primary key，所以可以形成动态表格，可以自动update.如果你仅仅是接受kafka的消息，scan模式的话，由于表格没有primary key，所以是不支持update,而只支持insert的。</div>
<div></div>
<div></div>
<div>所以针对update 和 delete 还需要单独处理….</div>
<div></div>
<div></div>
<div></div>
<div>在Table convert to DataStream一章里，使用了Tuple2&lt;Boolean, Row&gt;这个数据结构来区分Reactor Mode. 其中那个Boolean//   True is INSERT, false is DELETE.</div>
<div></div>
<div></div>
<div></div>
<div></div>
]]></content:encoded>
			<wfw:commentRss>https://huster.top/htmls/700.html/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>flink相关 &#8211; 读入kafka数据源</title>
		<link>https://huster.top/htmls/696.html</link>
		<comments>https://huster.top/htmls/696.html#respond</comments>
		<pubDate>Thu, 17 Oct 2019 08:47:14 +0000</pubDate>
		<dc:creator><![CDATA[龙安_任天兵]]></dc:creator>
				<category><![CDATA[flink]]></category>

		<guid isPermaLink="false">https://huster.top/?p=696</guid>
		<description><![CDATA[业务代码如下 : fsTableEnv.connect(        newKafka()          &#8230; <a href="https://huster.top/htmls/696.html" class="more-link">继续阅读<span class="screen-reader-text">“flink相关 &#8211; 读入kafka数据源”</span></a>]]></description>
				<content:encoded><![CDATA[<div>业务代码如下 :</div>
<div>fsTableEnv.connect(</div>
<div>       newKafka()</div>
<div>                .version(&#8220;0.11&#8221;)</div>
<div>                .topic(&#8220;xxxx&#8221;)</div>
<div>                .property(&#8220;bootstrap.servers”,”<a href="http://newdev-kafka1.ttbike.com.cn:9092/">x</a>xxxx:xxxx&#8221;)</div>
<div>                .property(&#8220;<a href="http://group.id/">group.id</a>&#8220;,&#8221;test4444&#8221;)</div>
<div>                .startFromEarliest()//测试需要</div>
<div>)</div>
<div>        .withSchema(newSchema().schema(newTableSchema(fields, types)))</div>
<div>        .withFormat(newJson().schema(newRowTypeInfo(types, fields)))</div>
<div>        .inAppendMode()</div>
<div>        .registerTableSource(“testTable”);</div>
<div></div>
<div></div>
<div>问题是：读出来的数据总是null</div>
<div>DataStream&lt;Row&gt; table = fsTableEnv.toAppendStream(fsTableEnv.sqlQuery(&#8220;select * from testTable&#8221;),Row.class);</div>
<div>table.print();</div>
<div></div>
<div></div>
<p><span id="more-696"></span></p>
<div></div>
<div></div>
<div><a href="https://huster.top/wp-content/uploads/2019/10/3.png"><img class="alignnone size-full wp-image-697" src="https://huster.top/wp-content/uploads/2019/10/3.png" alt="" width="1810" height="219" /></a></div>
<div></div>
<div></div>
<div>Kafka.java 这个类依然是一个配置收集器，就是为了收集一堆配置，然后给后面的工厂方法使用。依然通过FactoryService找到了Kafka011TableSourceSinkFactory这个工厂类。</div>
<div></div>
<div>Kafka011TableSourceSinkFactory : 作用是为了生成Kafka011TableSource这个TableSource。</div>
<div></div>
<div>Kafka011TableSource: 这个类的主要功效还是一个TableSource除了定义了TableSchema和ReturnType，最主要的是完成了他的核心功能，getDataStream()方法</div>
<div></div>
<div>FlinkKafkaConsumerBase : 由上一步的getDataStream方法，其实是为了生成这个类。这个类是一个SourceFunction，其中弹出信息的方法是run方法，我们继续看他的run方法</div>
<div></div>
<div></div>
<div>最后跟踪定位到convert那个地方，才发现，原来解析的时候，他使用的是完整的json schema，而不是仅仅是data里的field字段，例如你传给kafka的json schema是包含了data平级的那些字段,table,operation等。而我传过去的是data里的field字段，当然就错了。</div>
<div></div>
<div>{</div>
<div>    &#8220;schema&#8221;: “44444&#8243;,</div>
<div>    &#8220;table&#8221;: &#8220;safs&#8221;,</div>
<div>    &#8220;operation&#8221;: &#8220;INSERT&#8221;,</div>
<div>    &#8220;data&#8221;: {</div>
<div>        &#8220;start_time&#8221;: &#8220;2019-10-10 23:00:00&#8221;,</div>
<div>        &#8220;end_point&#8221;: &#8220;0101000020E61000008BDEA9807B545E4062D9CC21A9313F40&#8221;,</div>
<div>        &#8220;start_point&#8221;: &#8220;0101000020E61000002250FD8348575E40C284D1AC6C1F3F40&#8221;,</div>
<div>        &#8220;user_new_id&#8221;: 1200001917,</div>
<div>        &#8220;end_adcode&#8221;: null,</div>
<div>        &#8220;end_time&#8221;: null,</div>
<div>        &#8220;distance&#8221;: null,</div>
<div>        &#8220;seat_count&#8221;: 4</div>
<div>    },</div>
<div>    &#8220;operateTime&#8221;: 1570602631926</div>
<div>}</div>
<div></div>
<div></div>
<div>跟踪调试，发现具体的是在Kafka09Fetcher的runFetchLoop方法里有一段，final T value = deserializer.deserialize(record);解析出来就出错了，就空了。继续跟踪调试，发现是在 JsonRowDeserializationSchema类的</div>
<div>@Override</div>
<div>public Row deserialize(byte[] message) throws IOException {</div>
<div>   try {</div>
<div>      final JsonNode root = objectMapper.readTree(message);</div>
<div>      return (Row) runtimeConverter.convert(objectMapper, root);</div>
<div>   } catch (Throwable t) {</div>
<div>      throw new IOException(&#8220;Failed to deserialize JSON object.&#8221;, t);</div>
<div>   }</div>
<div>}</div>
<div>这段代码里，root解析出来是好的，然后他开始convert.</div>
<div></div>
<div>private DeserializationRuntimeConverter assembleRowConverter(</div>
<div>   String[] fieldNames,</div>
<div>   List&lt;DeserializationRuntimeConverter&gt; fieldConverters) {</div>
<div>   return (mapper, jsonNode) -&gt; {</div>
<div>      ObjectNode node = (ObjectNode) jsonNode;</div>
<div>      int arity = fieldNames.length;</div>
<div>      Row row = new Row(arity);</div>
<div>      for (int i = 0; i &lt; arity; i++) {</div>
<div>         String fieldName = fieldNames[i];</div>
<div>         JsonNode field = node.get(fieldName);</div>
<div>         Object convertField = convertField(mapper, fieldConverters.get(i), fieldName, field);</div>
<div>         row.setField(i, convertField);</div>
<div>      }</div>
<div>      return row;</div>
<div>   };</div>
<div>}</div>
<div></div>
<div>其中有这么一段JsonNode field = node.get(fieldName);他拿着你的fieldName去json数据里get,因为少了一个data层，所以就出错了。</div>
<div></div>
<div></div>
<div>这个kafka的连接器的依赖也挺奇葩的 。</div>
<div><a href="https://huster.top/wp-content/uploads/2019/10/4.png"><img class="alignnone size-full wp-image-698" src="https://huster.top/wp-content/uploads/2019/10/4.png" alt="" width="1141" height="132" /></a></div>
<div></div>
<div>0.11版本依赖0.10.再依赖0.9……..让人很迷惑.</div>
]]></content:encoded>
			<wfw:commentRss>https://huster.top/htmls/696.html/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>flink学习笔记(一)</title>
		<link>https://huster.top/htmls/691.html</link>
		<comments>https://huster.top/htmls/691.html#respond</comments>
		<pubDate>Thu, 17 Oct 2019 08:45:30 +0000</pubDate>
		<dc:creator><![CDATA[龙安_任天兵]]></dc:creator>
				<category><![CDATA[flink]]></category>

		<guid isPermaLink="false">https://huster.top/?p=691</guid>
		<description><![CDATA[架构  什么是flink flink是一个框架或者叫流式计算引擎，他可以用来处理有界(批处理，mapReduc &#8230; <a href="https://huster.top/htmls/691.html" class="more-link">继续阅读<span class="screen-reader-text">“flink学习笔记(一)”</span></a>]]></description>
				<content:encoded><![CDATA[<div>架构</div>
<ol>
<li>
<div> 什么是flink</div>
</li>
</ol>
<div>flink是一个框架或者叫流式计算引擎，他可以用来处理有界(批处理，mapReduce)数据和无界数据。他的特点是具有内存的速度和分布式的横向扩展能力。由于是分布式的，所以他可以处理任意规模的数据。另外，他是有状态的，表现为上一步的结算结果可以为下一步使用。</div>
<div></div>
<p><span id="more-691"></span></p>
<div>有界和无界的解释，见下面的图。</div>
<div><a href="https://huster.top/wp-content/uploads/2019/10/1.png"><img class="alignnone size-full wp-image-692" src="https://huster.top/wp-content/uploads/2019/10/1.png" alt="" width="2220" height="526" /></a></div>
<ol start="2">
<li>
<div>    如何解释内存般的访问速度。</div>
</li>
</ol>
<div>是指用户的逻辑始终需要不断的访问记录的状态，而这些状态被记录在内存里，这样访问就很快。当内存放不下的时候，会固化到硬盘里。另外，会定期的异步将当前的快照存放到硬盘里，以防止任务的重启能接着上次的终止的地方继续运算。如下图所示</div>
<div><a href="https://huster.top/wp-content/uploads/2019/10/2.png"><img class="alignnone size-full wp-image-693" src="https://huster.top/wp-content/uploads/2019/10/2.png" alt="" width="2784" height="630" /></a></div>
<div>运维</div>
<div>   1. flink的时间和特点</div>
<div>    flink提供了事件事件模式，即用事件发生时自身的时间戳来进行统计。好处是计算结果一定正确，坏处是可能有延迟。也就是即使是数据到达的不及时，也要保证结果的正确性。还有一种是处理时间模式，也就是用的是事件到达工作机的机器时间作为统计时间。这样做的好处是低延迟，坏处是可能结果不是那么的准确。</div>
<div>        2. flink的checkpoint和savepoint</div>
<div>checkpoint是异步的增量的持久化的保存着，是为了机器故障的恢复。而savepoint则是为了集群的迁移，版本的升级而设计。在流式应用中，一个很大的难点是如何在线平滑的升级你的flink应用(业务代码，业务发布)。如果你的业务代码都重新发布了，那以前的计算的中间结果如何承接过来。总不能你一升级，用户的点击数就从0开始计算吧？flink使用savepoint很好的解决了这个难题。另外，还支持flink版本的平滑升级。</div>
]]></content:encoded>
			<wfw:commentRss>https://huster.top/htmls/691.html/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
	</channel>
</rss>
